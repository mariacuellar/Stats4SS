<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Linear regression</title>

<script src="site_libs/header-attrs-2.10/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statistics for the Social Sciences (Crim 250)</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="EDA.html">EDA</a>
</li>
<li>
  <a href="Linear-regression.html">Linear regression</a>
</li>
<li>
  <a href="Assignments.html">Assignments</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Linear regression</h1>

</div>


<p>Created: 2021-10-19, Last compiled: 2021-10-19</p>
<p>Here we discuss how one should run a linear regression in R. There is a special emphasis on explaining what is a model, and how we use it to evaluate the relationship between two variables, x and y, in order to do proper inferences and predictions.</p>
<div id="dataset" class="section level1">
<h1>Dataset</h1>
<p>We will be using an example from the <code>cars</code> dataset in R. This comes with R in the <code>datasets</code> package.</p>
<pre class="r"><code># install.packages(&quot;datasets&quot;) # only run this once per session!
library(datasets)</code></pre>
<p>The data give the speed of cars (in miles per hour, mph) and the distances taken to stop (in miles). The data were recorded in the 1920s.</p>
</div>
<div id="scatterplot" class="section level1">
<h1>(Scatterplot)</h1>
<p>This is what the scatterplot of speed vs distance looks like.</p>
<pre class="r"><code>library(datasets)
plot(cars$speed, cars$dist,  main=&quot;Relationship between Speed and Stopping Distance for 50 Cars&quot;,
    xlab=&quot;Speed in mph&quot;, ylab=&quot;Stopping Distance in feet&quot;)</code></pre>
<p><img src="Linear-regression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>It kind of looks like there could be a linear relationship between the two variables, something like the blue line here. (Never mind how I plotted this blue line. I just included it here to show you that the line of best fit probably looks something like this blue line.)</p>
<pre class="r"><code>library(datasets)
reg.output.nc &lt;- lm(formula = dist ~ speed, data = cars)

plot(cars$speed, cars$dist,  main=&quot;Relationship between Speed and Stopping Distance for 50 Cars&quot;,
    xlab=&quot;Speed in mph&quot;, ylab=&quot;Stopping Distance in feet&quot;)
abline(reg.output.nc, col=&quot;blue&quot;)</code></pre>
<p><img src="Linear-regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>This blue line tells us there is a positive correlation between how fast the car is going and how far the car needs to go to stop. In fact, the correlation is</p>
<pre class="r"><code>cor(cars$speed, cars$dist)</code></pre>
<pre><code>## [1] 0.8068949</code></pre>
<p>So, the fact that the relationship looks linear tells us that perhaps a simple linear regression is a good model to fit to this dataset. That regression model will serve as a tool for us to be able to perform inferences (e.g. if I observe a higher stopping distance, is this associated with a higher speed? And if so by how much?) and make predictions (e.g. for a speed of 22 mph or 50 mph, neither of which are in the dataset, what is the estimated stopping distance?).</p>
</div>
<div id="linear-model" class="section level1">
<h1>Linear model</h1>
<div id="the-model" class="section level2 unlisted unnumbered">
<h2 class="unlisted unnumbered">The model</h2>
<ul>
<li><p>The linear regression model assumes that the <em>means</em> of the distributions of y’s for each x fall along the line, even though the individuals are scattered around it.</p></li>
<li><p>The <strong>model</strong> is <span class="math display">\[
\mu_y = \beta_0 + \beta_1 x.
\]</span> We use Greek letters to denote idealized models. Whenever we use linear regression, we’re assuming that this is actually how the data points are distributed.</p></li>
<li><p>Are the data really going to be distributed like this?</p></li>
</ul>
</div>
<div id="the-errors" class="section level2 unlisted unnumbered">
<h2 class="unlisted unnumbered">The errors</h2>
<ul>
<li><p>No, not all the individual y’s are at these means. Some are above, some below. So, like all models, this one makes <strong>errors</strong>. They are model errors so we call them <span class="math inline">\(\epsilon\)</span>.</p></li>
<li><p>When we include the errors, we can actually say that each individual y is along the line, with some variation, <span class="math display">\[y = \beta_0 + \beta_1 x + \epsilon,\]</span> where <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are each individual point’s <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values, the <span class="math inline">\(\beta\)</span>s are the <strong>parameters</strong> of the model (slope and intercept), and <span class="math inline">\(\epsilon\)</span> are the model errors that soak up the deviation from the model to the actual point. This equation is true for each data point.</p></li>
</ul>
</div>
<div id="the-regression-line" class="section level2 unlisted unnumbered">
<h2 class="unlisted unnumbered">The regression line</h2>
<ul>
<li><p>We estimate the <span class="math inline">\(\beta\)</span>s by finding a <strong>regression line</strong> <span class="math display">\[\hat{y} = b_0 + b_1 x,\]</span> as in the previous class. The <strong>residuals</strong>, <span class="math inline">\(e=y-\hat{y}\)</span> are the sample-based versions of the errors <span class="math inline">\(\epsilon\)</span>.</p></li>
<li><p>We use a method called <strong>least squares regression</strong>, which minimizes the vertical distance from the data points to the regression line (the sum of the squares of the residuals), to get reasonable estimates of the parameters of this model from a random sample of data.</p></li>
<li><p><strong>Important note</strong>: We don’t expect the assumptions to be exactly true, and we know that all models are wrong, but the linear model is often close enough to be very useful.</p></li>
</ul>
</div>
</div>
<div id="procedure-for-linear-regression" class="section level1">
<h1>Procedure for linear regression</h1>
<p>So, we have an idealized model that we’re going to use to see if there are interesting relationships between our variables. But, we’re not sure it’s the right model to use. Should we just go ahead and use it even if it’s the wrong model, and then later see if it worked? Doesn’t that seem dishonest? How can we use the model and the diagnostics properly?</p>
<p>We have some visual tests we can use before running the regression (does the scatterplot of y vs. x data look like it’s distributed like a line?) and diagnostics that we can use after running the regression. We’ll use these tools in the right order to run a linear model.</p>
<p>The following is a reasonable procedure from DVB. Note that although this seems like a simple algorithm, it’s not always the right thing to do. Remember this is statistics: you need a human deciding whether the assumptions you are making are too strong, and whether the diagnostics you’re checking look good enough. That’s why we go over the assumptions and the diagnostics later on. On a test, you should be able to respond to questions about whether you think each of the four assumptions is satisfied.</p>
<p><img src="images/Procedure%20for%20regression.png" /></p>
</div>
<div id="r-linear-regression-formula-and-output" class="section level1">
<h1>R linear regression formula and output</h1>
<p>Note: We are going to use a trick here called scaling the data. All this does is give us an easier time interpreting the y-intercept. You do not need to scale the data in most cases.</p>
<pre class="r"><code>cars$speed.c = scale(cars$speed, center=TRUE, scale=FALSE) # scaling the data
reg.output &lt;- lm(formula = dist ~ speed.c, data = cars) # running regression
summary(reg.output) # calling the summary of the fitted model</code></pre>
<pre><code>## 
## Call:
## lm(formula = dist ~ speed.c, data = cars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -29.069  -9.525  -2.272   9.215  43.201 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  42.9800     2.1750  19.761  &lt; 2e-16 ***
## speed.c       3.9324     0.4155   9.464 1.49e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.38 on 48 degrees of freedom
## Multiple R-squared:  0.6511, Adjusted R-squared:  0.6438 
## F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12</code></pre>
<p>The model above is achieved by using the <code>lm()</code> function in R and the output is called using the <code>summary()</code> function on the model. [Cite: Felipe Rego.]</p>
<ul>
<li><p><strong>Formula call</strong>: The first item shown in the output is the formula R used to fit the data. Note the simplicity in the syntax: the formula just needs the predictor (x) and the target/response variable (y), together with the data being used (dat).</p></li>
<li><p><strong>Residuals</strong>: The next item in the model output talks about the residuals. Residuals are essentially the difference between the actual observed response values (distance to stop dist in our case) and the response values that the model predicted.</p></li>
<li><p><strong>Coefficient - Estimate</strong>: The coefficient Estimate contains two rows; the first one is the intercept. The intercept, in our example, is essentially the expected value of the distance required for a car to stop when we consider the average speed of all cars in the dataset. In other words, it takes an average car in our dataset 42.98 feet to come to a stop. The second row in the Coefficients is the slope, or in our example, the effect speed has in distance required for a car to stop. The slope term in our model is saying that for every 1 mph increase in the speed of a car, the required distance to stop goes up by 3.9324088 feet. NOTE: This is not a causal interpretation. Just make sure you say that a higher value of speed is associated with higher values of distance, by this much, but don’t say that if you increase one the other one will increase.</p></li>
<li><p><strong>Coefficient - Standard Error</strong>: The coefficient Standard Error measures the average amount that the coefficient estimates vary from the actual average value of our response variable. We’d ideally want a lower number relative to its coefficients. In our example, we’ve previously determined that for every 1 mph increase in the speed of a car, the required distance to stop goes up by 3.9324088 feet. The Standard Error can be used to compute an estimate of the expected difference in case we ran the model again and again. In other words, we can say that the required distance for a car to stop can vary by 0.4155128 feet. The Standard Errors can also be used to compute confidence intervals and to statistically test the hypothesis of the existence of a relationship between speed and distance required to stop.</p></li>
<li><p><strong>Coefficient - t-value</strong>: The coefficient t-value is a measure of how many standard deviations our coefficient estimate is far away from 0. We want it to be far away from zero as this would indicate we could reject the null hypothesis - that is, we could declare a relationship between speed and distance exist. In our example, the t-statistic values are relatively far away from zero and are large relative to the standard error, which could indicate a relationship exists. In general, t-values are also used to compute p-values.</p></li>
<li><p><strong>Coefficient - Pr(&gt;t)</strong>: The Pr(&gt;t) acronym found in the model output relates to the probability of observing any value equal or larger than t. A small p-value indicates that it is unlikely we will observe a relationship between the predictor (speed) and response (dist) variables due to chance. Typically, a p-value of 5% or less is a good cut-off point. In our model example, the p-values are very close to zero. Note the ‘signif. Codes’ associated to each estimate. Three stars (or asterisks) represent a highly significant p-value. Consequently, a small p-value for the intercept and the slope indicates that we can reject the null hypothesis which allows us to conclude that there is a relationship between speed and distance.</p></li>
<li><p><strong>Residual Standard Error</strong>: Residual Standard Error is measure of the quality of a linear regression fit. Theoretically, every linear model is assumed to contain an error term E. Due to the presence of this error term, we are not capable of perfectly predicting our response variable (dist) from the predictor (speed) one. The Residual Standard Error is the average amount that the response (dist) will deviate from the true regression line. In our example, the actual distance required to stop can deviate from the true regression line by approximately 15.3795867 feet, on average. In other words, given that the mean distance for all cars to stop is 42.98 and that the Residual Standard Error is 15.3795867, we can say that the percentage error is (any prediction would still be off by) 35.78%. It’s also worth noting that the Residual Standard Error was calculated with 48 degrees of freedom. Simplistically, degrees of freedom are the number of data points that went into the estimation of the parameters used after taking into account these parameters (restriction). In our case, we had 50 data points and two parameters (intercept and slope).</p></li>
<li><p><strong>Multiple R-squared, Adjusted R-squared</strong>: The R-squared (R2) statistic provides a measure of how well the model is fitting the actual data. It takes the form of a proportion of variance. R2 is a measure of the linear relationship between our predictor variable (speed) and our response / target variable (dist). It always lies between 0 and 1 (i.e.: a number near 0 represents a regression that does not explain the variance in the response variable well and a number close to 1 does explain the observed variance in the response variable). In our example, the R2 we get is 0.6510794. Or roughly 65% of the variance found in the response variable (dist) can be explained by the predictor variable (speed). Step back and think: If you were able to choose any metric to predict distance required for a car to stop, would speed be one and would it be an important one that could help explain how distance would vary based on speed? I guess it’s easy to see that the answer would almost certainly be a yes. That why we get a relatively strong R2. Nevertheless, it’s hard to define what level of R2 is appropriate to claim the model fits well. Essentially, it will vary with the application and the domain studied.</p></li>
<li><p>A side note: In multiple regression settings, the R2 will always increase as more variables are included in the model. That’s why the adjusted R2 is the preferred measure as it adjusts for the number of variables considered.</p></li>
<li><p><strong>F-Statistic</strong>: F-statistic is a good indicator of whether there is a relationship between our predictor and the response variables. The further the F-statistic is from 1 the better it is. However, how much larger the F-statistic needs to be depends on both the number of data points and the number of predictors. Generally, when the number of data points is large, an F-statistic that is only a little bit larger than 1 is already sufficient to reject the null hypothesis (H0 : There is no relationship between speed and distance). The reverse is true as if the number of data points is small, a large F-statistic is required to be able to ascertain that there may be a relationship between predictor and response variables. In our example the F-statistic is 89.5671065 which is relatively larger than 1 given the size of our data.</p></li>
</ul>
<p>Now we revisit the assumptions.</p>
</div>
<div id="linearity-assumption" class="section level1">
<h1>1. Linearity assumption</h1>
<ul>
<li><p>This is satisfied if a scatterplot of x and y looks straight. If the true relationship between x and y is far from linear and we use a straight line to fit the data, our entire analysis will be useless, so we always check this first.</p></li>
<li><p><strong>How to check?</strong> You can see violations of this if you plot a scatterplot of the residuals against x or against the predicted values <span class="math inline">\(\hat{y}\)</span>. That plot will have a horizontal direction and should have no pattern if the condition is satisfied. You can think of the residuals as being estimates of the error terms. So anytime we’re looking at a plot that involves residuals, we’re doing so because we’re trying to assess whether some assumption about the errors appears to hold in our data.</p></li>
</ul>
</div>
<div id="residuals-vs.-x" class="section level1">
<h1>(Residuals vs. x)</h1>
<pre class="r"><code>plot(cars$speed.c, reg.output$residuals, ylim=c(-15,15), main=&quot;Residuals vs. x&quot;, xlab=&quot;x, Scaled speed&quot;, ylab=&quot;Residuals&quot;)
abline(h = 0, lty=&quot;dashed&quot;)</code></pre>
<p><img src="Linear-regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="residuals-vs.-fitted" class="section level1">
<h1>(Residuals vs. fitted)</h1>
<pre class="r"><code>plot(reg.output, which=1)</code></pre>
<p><img src="Linear-regression_files/figure-html/unnamed-chunk-7-1.png" width="480" /></p>
<ul>
<li>Looking at the Residuals vs Fitted plot (showing residuals on the y-axis and fitted y’s on the x-axis), we see that the red line (which is just a scatterplot smoother, showing the average value of the residuals at each value of fitted value) is quite flat. This tells us that there is no discernible non-linear trend to the residuals. Furthermore, the residuals appear to be equally variable across the entire range of fitted values. There is no indication of non-constant variance.</li>
</ul>
</div>
<div id="independence-assumption" class="section level1">
<h1>2. Independence assumption</h1>
<ul>
<li><p>The errors in the true underlying regression model (the <span class="math inline">\(\epsilon\)</span>s) must be independent of each other. There is no way to check this is true. We check displays of the regression residuals for evidence of patterns, trends, or clumping, any of which would suggest a failure of independence. e.g. For time series, the error our model makes today may be similar to the one it made for yesterday.</p></li>
<li><p><strong>How to check?</strong> You can see violations of this by plotting the residuals against x and looking for patterns (see plot above). Or, plot the residuals vs. the residuals offset or lagged by one time position. Neither plot should show patterns.</p></li>
<li><p>In our example this looks pretty good. We don’t need to plot the residuals vs. the lagged residuals because we don’t think there’s a time-series component in the data.</p></li>
</ul>
</div>
<div id="equal-variance-assumptionhomoscedasticity" class="section level1">
<h1>3. Equal variance assumption/homoscedasticity</h1>
<ul>
<li><p>The variability in y should be about the same for all values of x. The standard deviation of the residuals “pools” information across all of the individual distributions at each x-value, and pooled estimates are appropriate only when they combine information for groups with the same variance.</p></li>
<li><p><strong>How to check?</strong> A scatterplot of y against x offers a visual check (we did this all the way at the top). Be alert for a “fan” shape or other tendency for the variation to grow or shrink in one part of the scatterplot. Often, it is better to look at the residuals plotted against the predicted values <span class="math inline">\(\hat{y}\)</span>.</p></li>
</ul>
</div>
<div id="scale-location-plot" class="section level1">
<h1>(Scale-location plot)</h1>
<pre class="r"><code>plot(reg.output, which=3)</code></pre>
<p><img src="Linear-regression_files/figure-html/unnamed-chunk-8-1.png" width="480" /></p>
<ul>
<li><p>The scale-location plot is a more sensitive approach to looking for deviations from the constant variance assumption. If you see significant trends in the red line on this plot, it tells you that the residuals (and hence errors) have non-constant variance. That is, the assumption that all the errors have the same variance is not true.</p></li>
<li><p>When you see a flat line like what’s shown above, it means your errors have constant variance, like we want to see.</p></li>
</ul>
</div>
<div id="normal-population-assumption" class="section level1">
<h1>4. Normal population assumption</h1>
<ul>
<li><p>We assume the errors around the idealized regression line at each value of x follow a Normal model. <strong>Why?</strong> We need this assumption so we can use Student’s t-model for inference.</p></li>
<li><p>This assumption becomes less important as the same size grows because the model is about means and the Central Limit Theorem takes over.</p></li>
<li><p><strong>How to check?</strong> Nearly normal condition (qq plot) and outlier condition (Cook’s distance).</p></li>
</ul>
</div>
<div id="residuals-vs.-leverage-plot" class="section level1">
<h1>(Residuals vs. Leverage plot)</h1>
<pre class="r"><code>plot(reg.output, which=5)</code></pre>
<p><img src="Linear-regression_files/figure-html/unnamed-chunk-9-1.png" width="480" /></p>
<ul>
<li>There’s no single accepted definition for what consitutes an outlier. One possible definition is that an outlier is any point that isn’t approximated well by the model (has a large residual) and which significantly influences model fit (has large leverage). This is where the Residuals vs Leverage plot comes in.</li>
</ul>
</div>
<div id="normal-qq-plot" class="section level1">
<h1>(Normal qq plot)</h1>
<pre class="r"><code>plot(reg.output, which=2)</code></pre>
<p><img src="Linear-regression_files/figure-html/unnamed-chunk-10-1.png" width="480" /></p>
<ul>
<li><p>The Normal QQ plot helps us to assess whether the residuals are roughly normally distributed. If the residuals look far from normal we may be in trouble. In particular, if the residual tend to be larger in magnitude than what we would expect from the normal distribution, then our p-values and confidence intervals may be too optimisitic. i.e., we may fail to adequately account for the full variability of the data.</p></li>
<li><p>This qq plot is not great, especially at the top right. This tells us that the right tail of the distribution is probably “light” or smaller than usual for a normal distribution.</p></li>
<li><p>The images below are a guide that tells you what might be happening.</p></li>
</ul>
<p><img src="images/qqplot%20interpretations.png" /></p>
</div>
<div id="references" class="section level1 unlisted unnumbered">
<h1 class="unlisted unnumbered">References</h1>
<ul>
<li><p>DVB Chp 25</p></li>
<li><p><a href="https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R" class="uri">https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R</a></p></li>
<li><p><a href="https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html" class="uri">https://www.andrew.cmu.edu/user/achoulde/94842/homework/regression_diagnostics.html</a></p></li>
<li><p><a href="https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot" class="uri">https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot</a></p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
